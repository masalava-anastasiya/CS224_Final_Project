{
  "cells": [
    {
      "cell_type": "code",
      "id": "OFdQjO5FxSyEOMQkWOApMthh",
      "metadata": {
        "tags": [],
        "id": "OFdQjO5FxSyEOMQkWOApMthh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1cd563-5eeb-4a77-8cc3-94e6ffe0cae0",
        "collapsed": true
      },
      "source": [
        "!pip install numpy pandas networkx scikit-learn node2vec tqdm\n",
        "!pip install torch torch-geometric torch-scatter torch-sparse torch-geometric-temporal\n",
        "!pip install google-cloud-bigquery\n",
        "!pip install google-cloud-storage"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: node2vec in /usr/local/lib/python3.12/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (4.4.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (2.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-geometric-temporal\n",
            "  Using cached torch_geometric_temporal-0.56.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (3.0.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Using cached torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "Using cached torch_geometric_temporal-0.56.2-py3-none-any.whl (102 kB)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl size=3806389 sha256=943033add0679941f36546c4d6f9538c7cccf6f316ba4bc98d62a4d5d3449e4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/20/50/44800723f57cd798630e77b3ec83bc80bd26a1e3dc3a672ef5\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl size=2874601 sha256=0dadfdc68dea40e85aa6c536ab5b942028087c9cc077a1abd6d9bd9e655285f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/fa/21/bd1d78ce1629aec4ecc924a63b82f6949dda484b6321eac6f2\n",
            "Successfully built torch-scatter torch-sparse\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-geometric, torch-geometric-temporal\n",
            "Successfully installed torch-geometric-2.7.0 torch-geometric-temporal-0.56.2 torch-scatter-2.1.2 torch-sparse-0.6.18\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.28.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.32.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (6.33.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.28.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.32.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.71.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (6.33.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.10.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "TORCH_VERSION = torch.__version__.split('+')[0]\n",
        "CUDA_VERSION = torch.version.cuda\n",
        "if CUDA_VERSION:\n",
        "    CUDA_VERSION = \"cu\" + CUDA_VERSION.replace('.', '')\n",
        "else:\n",
        "    CUDA_VERSION = \"cpu\""
      ],
      "metadata": {
        "id": "c45ndKEH-EnU"
      },
      "id": "c45ndKEH-EnU",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.auth import default\n",
        "creds, _ = default()"
      ],
      "metadata": {
        "id": "-5vxc-_HRyQm"
      },
      "id": "-5vxc-_HRyQm",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKCWxo0zK5s_",
        "outputId": "b520e3b9-522c-4c53-d1b8-47a7d7d394f4"
      },
      "id": "gKCWxo0zK5s_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://cs224w-mimic-data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Preprocessed Data"
      ],
      "metadata": {
        "id": "oqVm2KovdDQ4"
      },
      "id": "oqVm2KovdDQ4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve graph from MIMIC preprocessing\n",
        "\n",
        "from google.cloud import storage\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "bucket_name = \"cs224w-mimic-data\"\n",
        "edges_path = \"disease_edges.csv\"\n",
        "nodes_path = \"disease_nodes.csv\"\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "edges_blob = bucket.blob(edges_path)\n",
        "nodes_blob = bucket.blob(nodes_path)\n",
        "\n",
        "edges_bytes = edges_blob.download_as_bytes()\n",
        "nodes_bytes = nodes_blob.download_as_bytes()\n",
        "\n",
        "edges_df = pd.read_csv(io.BytesIO(edges_bytes), parse_dates=[\"timestamp\"])\n",
        "nodes_df = pd.read_csv(io.BytesIO(nodes_bytes))\n",
        "\n",
        "print(edges_df.head())\n",
        "print(nodes_df.head())\n",
        "print(\"Total edges:\", len(edges_df))\n",
        "print(\"Total unique diseases:\", len(nodes_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzYsvK7ocnrD",
        "outputId": "4d1a0b67-c338-4b5e-8cdf-f43c4acd8a00"
      },
      "id": "RzYsvK7ocnrD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     src    dst           timestamp\n",
            "0    496  30981 2180-05-06 22:23:00\n",
            "1  30981  07070 2180-05-06 22:23:00\n",
            "2  07070   5723 2180-05-06 22:23:00\n",
            "3   5723   5715 2180-05-06 22:23:00\n",
            "4   5715  78959 2180-05-06 22:23:00\n",
            "  disease_code\n",
            "0          496\n",
            "1        30981\n",
            "2        07070\n",
            "3         5723\n",
            "4         5715\n",
            "Total edges: 6141197\n",
            "Total unique diseases: 28562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Train/Test Split\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# sort by timestamp\n",
        "edges_df = edges_df.sort_values(\"timestamp\")\n",
        "\n",
        "# Create a chronological 70/15/15 train/validation/test split\n",
        "# Resets dataframe indexing to sorted order\n",
        "train_ratio = 0.70\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "train_end_idx = int(train_ratio * len(edges_df))\n",
        "val_end_idx = int((train_ratio + val_ratio) * len(edges_df))\n",
        "\n",
        "# Create three splits\n",
        "train_edges_df = edges_df.iloc[:train_end_idx].reset_index(drop=True)\n",
        "val_edges_df = edges_df.iloc[train_end_idx:val_end_idx].reset_index(drop=True)\n",
        "test_edges_df = edges_df.iloc[val_end_idx:].reset_index(drop=True)\n",
        "\n",
        "# Verify splits\n",
        "total_edges = len(edges_df)\n",
        "train_pct = len(train_edges_df) / total_edges * 100\n",
        "val_pct = len(val_edges_df) / total_edges * 100\n",
        "test_pct = len(test_edges_df) / total_edges * 100\n",
        "\n",
        "print(f\"Total edges: {total_edges:,}\")\n",
        "print(f\"Train: {len(train_edges_df):,} ({train_pct:.1f}%)\")\n",
        "print(f\"Val:   {len(val_edges_df):,} ({val_pct:.1f}%)\")\n",
        "print(f\"Test:  {len(test_edges_df):,} ({test_pct:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Show temporal boundaries\n",
        "print(\"Temporal boundaries:\")\n",
        "print(f\"Train: {train_edges_df['timestamp'].min()} to {train_edges_df['timestamp'].max()}\")\n",
        "print(f\"Val:   {val_edges_df['timestamp'].min()} to {val_edges_df['timestamp'].max()}\")\n",
        "print(f\"Test:  {test_edges_df['timestamp'].min()} to {test_edges_df['timestamp'].max()}\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3Zja9p4LnjI",
        "outputId": "3b2b39d2-087b-4931-c2b8-b34a095f2c94"
      },
      "id": "G3Zja9p4LnjI",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total edges: 6,141,197\n",
            "Train: 4,298,837 (70.0%)\n",
            "Val:   921,180 (15.0%)\n",
            "Test:  921,180 (15.0%)\n",
            "\n",
            "Temporal boundaries:\n",
            "Train: 2105-10-04 17:26:00 to 2171-12-07 13:52:00\n",
            "Val:   2171-12-07 13:52:00 to 2183-12-21 19:57:00\n",
            "Test:  2183-12-21 19:57:00 to 2214-12-15 19:11:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- Generate positive samples -------------\n",
        "train_pos_df = train_edges_df.copy()\n",
        "val_pos_df = val_edges_df.copy()\n",
        "test_pos_df = test_edges_df.copy()\n",
        "\n",
        "# Convert to list format (for some baseline methods)\n",
        "train_pos = train_edges_df[[\"src\", \"dst\", \"timestamp\"]].values.tolist()\n",
        "val_pos = val_edges_df[[\"src\", \"dst\", \"timestamp\"]].values.tolist()\n",
        "test_pos = test_edges_df[[\"src\", \"dst\", \"timestamp\"]].values.tolist()\n",
        "\n",
        "print(f\"Positive samples:\")\n",
        "print(f\" Train: {len(train_pos):,}\")\n",
        "print(f\" Val:   {len(val_pos):,}\")\n",
        "print(f\" Test:  {len(test_pos):,}\")\n",
        "print()\n",
        "\n",
        "# ---------- Generate negative samples ----------\n",
        "\n",
        "def negative_sampling(pos_edges_df, all_nodes, exclude_edges_df=None,\n",
        "                           n_negatives_per_positive=1, seed=42):\n",
        "    \"\"\"\n",
        "    Strategy:\n",
        "    1. Generate many candidate negatives in large batches (vectorized)\n",
        "    2. Filter out positives using set operations\n",
        "    3. Sample exactly n negatives per positive\n",
        "\n",
        "    Args:\n",
        "        pos_edges_df: Positive edges DataFrame with columns ['src', 'dst', 'timestamp']\n",
        "        all_nodes: Array of all disease codes\n",
        "        exclude_edges_df: Edges to exclude from negatives\n",
        "        n_negatives_per_positive: Number of negatives per positive\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    n_pos = len(pos_edges_df)\n",
        "    needed = n_pos * n_negatives_per_positive\n",
        "\n",
        "    if exclude_edges_df is None:\n",
        "        exclude_set = set(zip(pos_edges_df[\"src\"], pos_edges_df[\"dst\"]))\n",
        "    else:\n",
        "        exclude_set = set(zip(exclude_edges_df[\"src\"], exclude_edges_df[\"dst\"]))\n",
        "\n",
        "    all_nodes = np.array(all_nodes)\n",
        "    n_nodes = len(all_nodes)\n",
        "\n",
        "    print(f\"Need {needed:,} negatives... generating candidates...\")\n",
        "\n",
        "    # Generate large batch\n",
        "    oversample_factor = 3\n",
        "    n_candidates = needed * oversample_factor\n",
        "    src_candidates = np.random.choice(all_nodes, size=n_candidates)\n",
        "    dst_candidates = np.random.choice(all_nodes, size=n_candidates)\n",
        "\n",
        "    # Filter self-loops\n",
        "    mask = src_candidates != dst_candidates\n",
        "\n",
        "    # Build array of tuples for filtering\n",
        "    src_str = src_candidates.astype(str)\n",
        "    dst_str = dst_candidates.astype(str)\n",
        "    pairs = np.char.add(src_str, np.char.add(\",\", dst_str))\n",
        "\n",
        "    exclude_pairs = set([f\"{s},{d}\" for (s, d) in exclude_set])\n",
        "    mask &= np.array([p not in exclude_pairs for p in pairs])\n",
        "\n",
        "    # Select valid negatives\n",
        "    valid_src = src_candidates[mask]\n",
        "    valid_dst = dst_candidates[mask]\n",
        "\n",
        "    # Take exactly the first `needed`\n",
        "    neg_src = valid_src[:needed]\n",
        "    neg_dst = valid_dst[:needed]\n",
        "\n",
        "    neg_df = pd.DataFrame({\n",
        "        \"src\": neg_src,\n",
        "        \"dst\": neg_dst,\n",
        "        \"timestamp\": None\n",
        "    })\n",
        "\n",
        "    print(f\"Generated {len(neg_df):,} negatives.\")\n",
        "    return neg_df\n",
        "\n",
        "\n",
        "all_diseases = nodes_df[\"disease_code\"].values\n",
        "\n",
        "print(f\"Generating training negative samples...\")\n",
        "train_neg_df = negative_sampling(\n",
        "    train_pos_df,\n",
        "    all_diseases,\n",
        "    exclude_edges_df=train_edges_df,  # exclude pos train examples\n",
        "    n_negatives_per_positive=1\n",
        ")\n",
        "\n",
        "print(f\"Generating validation negative samples...\")\n",
        "train_and_val = pd.concat([train_edges_df, val_edges_df])\n",
        "val_neg_df = negative_sampling(\n",
        "    val_pos_df,\n",
        "    all_diseases,\n",
        "    exclude_edges_df=train_and_val  # exclude pos train + validation examples\n",
        ")\n",
        "\n",
        "print(f\"Generating testing negative samples...\")\n",
        "test_neg_df = negative_sampling(\n",
        "    test_pos_df,\n",
        "    all_diseases,\n",
        "    exclude_edges_df=edges_df  # exclude all positive examples\n",
        ")\n",
        "\n",
        "print()\n",
        "print(f\"Negative samples generated:\")\n",
        "print(f\" Train: {len(train_neg_df):,}\")\n",
        "print(f\" Val:   {len(val_neg_df):,}\")\n",
        "print(f\" Test:  {len(test_neg_df):,}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TpD513rQMX4",
        "outputId": "03c0f00e-4384-4f5f-c209-0e8d09ee1cb4"
      },
      "id": "8TpD513rQMX4",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples:\n",
            " Train: 4,298,837\n",
            " Val:   921,180\n",
            " Test:  921,180\n",
            "\n",
            "Generating training negative samples...\n",
            "Need 4,298,837 negatives... generating candidates...\n",
            "Generated 4,298,837 negatives.\n",
            "Generating validation negative samples...\n",
            "Need 921,180 negatives... generating candidates...\n",
            "Generated 921,180 negatives.\n",
            "Generating testing negative samples...\n",
            "Need 921,180 negatives... generating candidates...\n",
            "Generated 921,180 negatives.\n",
            "\n",
            "Negative samples generated:\n",
            " Train: 4,298,837\n",
            " Val:   921,180\n",
            " Test:  921,180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def save_splits_local(train_pos_df, train_neg_df,\n",
        "                     val_pos_df, val_neg_df,\n",
        "                     test_pos_df, test_neg_df,\n",
        "                     save_dir=\"/content/data_splits\"):\n",
        "    \"\"\"\n",
        "    Save all splits to local files (useful for Colab session)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    print(\"Saving splits from local files\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save positive edges\n",
        "    train_pos_df.to_csv(f\"{save_dir}/train_pos.csv\", index=False)\n",
        "    val_pos_df.to_csv(f\"{save_dir}/val_pos.csv\", index=False)\n",
        "    test_pos_df.to_csv(f\"{save_dir}/test_pos.csv\", index=False)\n",
        "\n",
        "    # Save negative edges\n",
        "    train_neg_df.to_csv(f\"{save_dir}/train_neg.csv\", index=False)\n",
        "    val_neg_df.to_csv(f\"{save_dir}/val_neg.csv\", index=False)\n",
        "    test_neg_df.to_csv(f\"{save_dir}/test_neg.csv\", index=False)\n",
        "\n",
        "    print(f\"✓ Saved 6 files to {save_dir}/\")\n",
        "    print(f\"  train_pos.csv: {len(train_pos_df):,} rows\")\n",
        "    print(f\"  train_neg.csv: {len(train_neg_df):,} rows\")\n",
        "    print(f\"  val_pos.csv:   {len(val_pos_df):,} rows\")\n",
        "    print(f\"  val_neg.csv:   {len(val_neg_df):,} rows\")\n",
        "    print(f\"  test_pos.csv:  {len(test_pos_df):,} rows\")\n",
        "    print(f\"  test_neg.csv:  {len(test_neg_df):,} rows\")\n",
        "\n",
        "    # Calculate total size\n",
        "    total_size = sum(os.path.getsize(f\"{save_dir}/{f}\")\n",
        "                    for f in os.listdir(save_dir)) / (1024**2)\n",
        "    print(f\"  Total size: {total_size:.1f} MB\")\n",
        "    print()\n",
        "\n",
        "def load_splits_local(save_dir=\"/content/splits\"):\n",
        "    \"\"\"\n",
        "    Load all splits from local files\n",
        "    \"\"\"\n",
        "    print(\"Load splits from local files\")\n",
        "\n",
        "    train_pos_df = pd.read_csv(f\"{save_dir}/train_pos.csv\", parse_dates=[\"timestamp\"])\n",
        "    val_pos_df = pd.read_csv(f\"{save_dir}/val_pos.csv\", parse_dates=[\"timestamp\"])\n",
        "    test_pos_df = pd.read_csv(f\"{save_dir}/test_pos.csv\", parse_dates=[\"timestamp\"])\n",
        "\n",
        "    train_neg_df = pd.read_csv(f\"{save_dir}/train_neg.csv\")\n",
        "    val_neg_df = pd.read_csv(f\"{save_dir}/val_neg.csv\")\n",
        "    test_neg_df = pd.read_csv(f\"{save_dir}/test_neg.csv\")\n",
        "\n",
        "    print(f\"✓ Loaded 6 files from {save_dir}/\")\n",
        "    print(f\"  train_pos: {len(train_pos_df):,} rows\")\n",
        "    print(f\"  train_neg: {len(train_neg_df):,} rows\")\n",
        "    print(f\"  val_pos:   {len(val_pos_df):,} rows\")\n",
        "    print(f\"  val_neg:   {len(val_neg_df):,} rows\")\n",
        "    print(f\"  test_pos:  {len(test_pos_df):,} rows\")\n",
        "    print(f\"  test_neg:  {len(test_neg_df):,} rows\")\n",
        "    print()\n",
        "\n",
        "    return train_pos_df, train_neg_df, val_pos_df, val_neg_df, test_pos_df, test_neg_df\n",
        "\n",
        "save_splits_local(train_pos_df, train_neg_df, val_pos_df, val_neg_df,test_pos_df, test_neg_df,save_dir=\"/content/data_splits\")"
      ],
      "metadata": {
        "id": "phdEwYU60egt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16ea5d1-f0b9-49b2-a7db-63dc83680186"
      },
      "id": "phdEwYU60egt",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving splits from local files\n",
            "✓ Saved 6 files to /content/data_splits/\n",
            "  train_pos.csv: 4,298,837 rows\n",
            "  train_neg.csv: 4,298,837 rows\n",
            "  val_pos.csv:   921,180 rows\n",
            "  val_neg.csv:   921,180 rows\n",
            "  test_pos.csv:  921,180 rows\n",
            "  test_neg.csv:  921,180 rows\n",
            "  Total size: 263.2 MB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos_df, train_neg_df, val_pos_df, val_neg_df, test_pos_df, test_neg_df = load_splits_local() # run when need"
      ],
      "metadata": {
        "id": "Glc5_qmzOPkS"
      },
      "id": "Glc5_qmzOPkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Create validation set with labels ---------\n",
        "val_pos_w_label = val_pos_df[[\"src\", \"dst\"]].copy()\n",
        "val_pos_w_label[\"label\"] = 1\n",
        "\n",
        "val_neg_w_label = val_neg_df[[\"src\", \"dst\"]].copy()\n",
        "val_neg_w_label[\"label\"] = 0\n",
        "\n",
        "val_all = pd.concat([val_pos_w_label, val_neg_w_label], ignore_index=True)\n",
        "\n",
        "# Test set with labels\n",
        "test_pos_w_label = test_pos_df[[\"src\", \"dst\"]].copy()\n",
        "test_pos_w_label[\"label\"] = 1\n",
        "\n",
        "test_neg_w_label = test_neg_df[[\"src\", \"dst\"]].copy()\n",
        "test_neg_w_label[\"label\"] = 0\n",
        "\n",
        "test_all = pd.concat([test_pos_w_label, test_neg_w_label], ignore_index=True)\n",
        "\n",
        "# Shuffle for better evaluation (optional)\n",
        "val_all = val_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_all = test_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Labeled datasets:\")\n",
        "print(f\"  Validation: {len(val_all):,} edges ({val_all['label'].sum():,} positive, \"\n",
        "      f\"{(val_all['label'] == 0).sum():,} negative)\")\n",
        "print(f\"  Test:       {len(test_all):,} edges ({test_all['label'].sum():,} positive, \"\n",
        "      f\"{(test_all['label'] == 0).sum():,} negative)\")\n",
        "print()\n",
        "\n",
        "\n",
        "# ---------- Create training graph ------------\n",
        "\n",
        "G_train = nx.DiGraph()\n",
        "G_train.add_edges_from(train_edges_df[[\"src\", \"dst\"]].values)\n",
        "\n",
        "print(f\"Training graph statistics:\")\n",
        "print(f\"  Nodes: {G_train.number_of_nodes():,}\")\n",
        "print(f\"  Edges: {G_train.number_of_edges():,}\")\n",
        "print(f\"  Density: {nx.density(G_train):.6f}\")\n",
        "\n",
        "# Check for disconnected components\n",
        "if nx.is_weakly_connected(G_train):\n",
        "    print(f\"  Graph is weakly connected\")\n",
        "else:\n",
        "    num_components = nx.number_weakly_connected_components(G_train)\n",
        "    print(f\"  Graph has {num_components} weakly connected components\")\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "Lcs1BhCtpoKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57854a2a-8b70-4245-8043-7c0a068e4374"
      },
      "id": "Lcs1BhCtpoKB",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeled datasets:\n",
            "  Validation: 1,842,360 edges (921,180 positive, 921,180 negative)\n",
            "  Test:       1,842,360 edges (921,180 positive, 921,180 negative)\n",
            "\n",
            "Training graph statistics:\n",
            "  Nodes: 26,508\n",
            "  Edges: 1,527,244\n",
            "  Density: 0.002174\n",
            "  Graph is weakly connected\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline: Completely Random"
      ],
      "metadata": {
        "id": "dZZWyq9OwKxt"
      },
      "id": "dZZWyq9OwKxt"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "diseases = list(nodes_df[\"disease_code\"])\n",
        "y_true = test_all[\"label\"].values\n",
        "y_score_random = [random.random() for _ in range(len(y_true))]\n",
        "auc_random = roc_auc_score(y_true, y_score_random)\n",
        "ap_random = average_precision_score(y_true, y_score_random)\n",
        "\n",
        "hits = 0\n",
        "total = len(test_pos)\n",
        "for src, dst, _ in test_pos:\n",
        "    random_10_diseases = random.choices(diseases, k=10)\n",
        "    if dst in random_10_diseases:\n",
        "        hits += 1\n",
        "hits_at_10_random = hits / total\n",
        "\n",
        "print(\"Baseline: Completely Random\")\n",
        "print(\"ROC AUC:\", auc_random)\n",
        "print(\"Average Precision:\", ap_random)\n",
        "print(\"Hits@10:\", hits_at_10_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R67AC84iwNh9",
        "outputId": "ddeb609b-87bc-43eb-f2c4-daf786a29d95"
      },
      "id": "R67AC84iwNh9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: Completely Random\n",
            "ROC AUC: 0.4999290437408782\n",
            "Average Precision: 0.5002865647195095\n",
            "Hits@10: 0.0003272975965609327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline: Most Frequent Next Hop"
      ],
      "metadata": {
        "id": "Zgq-z1TZLHF4"
      },
      "id": "Zgq-z1TZLHF4"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# count transition per src -> dst pair\n",
        "transition_counts = train_pos_df.groupby([\"src\", \"dst\"]).size().reset_index(name=\"count\")\n",
        "\n",
        "# build transition frequency dictionary from training edges\n",
        "next_disease_freq = defaultdict(dict)\n",
        "for _, row in transition_counts.iterrows():\n",
        "    next_disease_freq[row[\"src\"]][row[\"dst\"]] = row[\"count\"]\n",
        "\n",
        "print(f\"Number of src diseases: {len(next_disease_freq)}\")\n",
        "\n",
        "total_edges = len(edges_df)\n",
        "def most_frequent_next_hop_score(src, dst):\n",
        "    # Just return the actual frequency/probability\n",
        "    freq = next_disease_freq[src].get(dst, 0)\n",
        "    total_from_src = sum(next_disease_freq[src].values())\n",
        "    if total_from_src == 0:\n",
        "        return 0\n",
        "    return freq / total_from_src  # P(dst | src)\n",
        "\n",
        "# score edges by how common they are\n",
        "test_all[\"score_most_common\"] = test_all.apply(\n",
        "    lambda row: most_frequent_next_hop_score(row[\"src\"], row[\"dst\"]),\n",
        "    axis=1\n",
        ")\n",
        "print(f\"Snippet of test_all: {test_all[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt8JfGJPUkcE",
        "outputId": "865dd9c7-6590-4a1b-df73-4fbb3e17ac9f"
      },
      "id": "wt8JfGJPUkcE",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of src diseases: 26138\n",
            "Snippet of test_all:        src      dst  label  score_most_common\n",
            "0  T82897A    H7092      0           0.000000\n",
            "1    V1271    V6284      1           0.000926\n",
            "2     E785     I255      1           0.002132\n",
            "3    66951  S30860A      0           0.000000\n",
            "4   I97120  V584XXA      0           0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# evaluate baseline\n",
        "y_true = test_all[\"label\"].values\n",
        "y_score = test_all[\"score_most_common\"].values\n",
        "top10_next = dict()\n",
        "for src, freqs in next_disease_freq.items():\n",
        "  top10_dst = sorted(freqs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "  top10_next[src] = top10_dst\n",
        "top10_next = {src: {dst for dst, _ in lst} for src, lst in top10_next.items()}\n",
        "\n",
        "auc = roc_auc_score(y_true, y_score)\n",
        "ap = average_precision_score(y_true, y_score)\n",
        "\n",
        "hits = 0\n",
        "total = len(test_pos)\n",
        "for src, dst, _ in test_pos:\n",
        "    if src in top10_next and dst in top10_next[src]:\n",
        "        hits += 1\n",
        "\n",
        "hits_at_10 = hits / total\n",
        "\n",
        "\n",
        "print(\"Baseline: Most Frequent Next Hop\")\n",
        "print(\"ROC AUC:\", auc)\n",
        "print(\"Average Precision:\", ap)\n",
        "print(\"Hits@10:\", hits_at_10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w-9-dzlajkX",
        "outputId": "13d9bdef-d5d8-4162-b6ea-ea1285c73721"
      },
      "id": "6w-9-dzlajkX",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: Most Frequent Next Hop\n",
            "ROC AUC: 0.879977854490979\n",
            "Average Precision: 0.879977854490979\n",
            "Hits@10: 0.17548361883670943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline: Degree Aware"
      ],
      "metadata": {
        "id": "kkxbgFiCmzpb"
      },
      "id": "kkxbgFiCmzpb"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# in-degree from train edges\n",
        "in_deg = train_edges_df[\"dst\"].value_counts()  # disease_code -> count\n",
        "max_deg = in_deg.max()\n",
        "\n",
        "def degree_aware_random_score(src, dst):\n",
        "    deg = in_deg.get(dst, 0)\n",
        "    # normalized degree\n",
        "    deg_norm = deg / max_deg\n",
        "    # random score but scaled a bit by degree\n",
        "    return deg_norm\n",
        "\n",
        "# score edges by in degree freq\n",
        "test_all[\"score_deg_rand\"] = test_all.apply(\n",
        "    lambda row: degree_aware_random_score(row[\"src\"], row[\"dst\"]),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "TL2L7kkXo0Fv"
      },
      "id": "TL2L7kkXo0Fv",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "y_true = test_all[\"label\"].values\n",
        "y_score = test_all[\"score_deg_rand\"].values\n",
        "\n",
        "auc = roc_auc_score(y_true, y_score)\n",
        "ap  = average_precision_score(y_true, y_score)\n",
        "\n",
        "print(\"Baseline: Degree Aware\")\n",
        "print(\"ROC AUC:\", auc)\n",
        "print(\"AP:\", ap)\n",
        "\n",
        "def compute_hits_at_k(test_df, score_col, k=10):\n",
        "    \"\"\"\n",
        "    test_df: DataFrame with columns ['src', 'dst', 'label', score_col]\n",
        "    score_col: name of the column with scores (e.g., 'score_deg_rand')\n",
        "    k: cutoff for Hits@K\n",
        "    \"\"\"\n",
        "    hits = []\n",
        "\n",
        "    for src, group in test_df.groupby(\"src\"):\n",
        "        # sort by model score descending\n",
        "        group_sorted = group.sort_values(score_col, ascending=False)\n",
        "        top_k = group_sorted.head(k)\n",
        "\n",
        "        # hit if any of the top-k edges is actually positive\n",
        "        hit = (top_k[\"label\"] == 1).any()\n",
        "        hits.append(int(hit))\n",
        "\n",
        "    return np.mean(hits)\n",
        "\n",
        "hits10 = compute_hits_at_k(test_all, score_col=\"score_deg_rand\", k=10)\n",
        "print(\"Hits@10:\", hits10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tu-AhhfpW32",
        "outputId": "616e0f29-fa3c-4937-f6f5-22f13c3f939b"
      },
      "id": "7tu-AhhfpW32",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: Degree Aware\n",
            "ROC AUC: 0.9654606428395175\n",
            "AP: 0.9644292119282976\n",
            "Hits@10: 0.572683985715286\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "cs224w_final_project",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}